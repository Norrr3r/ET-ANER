{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Norrr3r/ET-ANER/blob/main/1_Test_for_Majority_and_Weighted_voting_CANER_Corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsdz6ufUXS1l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW8WVaPBdlyP",
        "outputId": "2ad32b97-8ae9-4e59-e0a3-5d1b503b9f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41.0/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m891.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from seqeval==1.2.2) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval==1.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (3.6.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=23fc809ceae3914a3f14eff91416502417eccfcc0a0de3a4687fba136cd10c86\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JnhNVHAjCAnt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8DW6OGXUdquL"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score as f1,\n",
        "    accuracy_score)\n",
        "from seqeval.scheme import IOB2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l4xtz25GJZ2K"
      },
      "outputs": [],
      "source": [
        "def combine_predictions_for_hard_voting (files):\n",
        " #out = open(\"out.txt\", \"r+\")\n",
        " bst='\\t'\n",
        " list1 = [[] for h in range(25950)]\n",
        " dic ={}\n",
        "\n",
        "\n",
        " i=0  #index for files\n",
        " for file in files:\n",
        "    #index = 3\n",
        "    with open( file, 'r', encoding='utf-8') as f:\n",
        "        k=0   #index for lines\n",
        "        for line in f:\n",
        "\n",
        "            if not line.strip():\n",
        "              list1[k].append([''])\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                token, gold_tag,predicted_tag = line[0], line[1],line[2]\n",
        "                if i==0:\n",
        "                   list1[k].append([token])\n",
        "                   list1[k].append([gold_tag])\n",
        "                   list1[k].append([predicted_tag])\n",
        "                #else:\n",
        "                 #   list1[k].append([predicted_tag])\n",
        "\n",
        "                elif any(token in sublist for sublist in list1[k]):\n",
        "                   list1[k].append([predicted_tag])\n",
        "                else:\n",
        "                   dic[token] = predicted_tag\n",
        "\n",
        "            k+=1\n",
        "    i+=1\n",
        "\n",
        " for token, tag in dic.items():\n",
        "       for i, sublist in enumerate(list1):\n",
        "            if any(token in sublist for sublist in list1[i]):\n",
        "               list1[i].append([tag])\n",
        "               break\n",
        "\n",
        "\n",
        " with open('/content/Predictions_test.txt', 'w', encoding='utf-8') as f:\n",
        "    for kline in list1:\n",
        "        for item in kline:\n",
        "            f.write( bst.join(item) + '\\t')\n",
        "        f.write('\\n')\n",
        "#    for token , ptags in dic.items():\n",
        " #    f.write( f\"{token} \\t {ptags} \\n\")\n",
        "\n",
        " file_name = '/content/Predictions_test.txt'\n",
        " return file_name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yM13iwzRKIhY"
      },
      "outputs": [],
      "source": [
        "def Hard_Voting (files):\n",
        " predicted_tags = []\n",
        " vote_counts = {}\n",
        " list2=[]\n",
        " file_name = combine_predictions_for_hard_voting(files)\n",
        " with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "   for line in f:\n",
        "            line = line.strip().split()\n",
        "\n",
        "            if len(line)==0:\n",
        "              continue\n",
        "\n",
        "            elif len(line)==3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              pred_tag = line[2]\n",
        "              list2.append((token ,gold_tag, pred_tag))\n",
        "\n",
        "            elif len(line) >3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "\n",
        "              for tag in line[2:]:\n",
        "                if tag not in vote_counts:\n",
        "                   vote_counts[tag] = 0\n",
        "                vote_counts[tag] += 1\n",
        "              pred_tag = max(vote_counts, key=vote_counts.get)\n",
        "              list2.append((token ,gold_tag, pred_tag))\n",
        "              vote_counts.clear()\n",
        "\n",
        "\n",
        " with open('Compined_Predictions.txt', 'w', encoding='utf-8') as f:\n",
        "    for token, gold_tag, pred_tag in list2:\n",
        "        f.write(f\"{token}\\t{gold_tag}\\t{pred_tag}\\n\")\n",
        "\n",
        " file_voting = 'Compined_Predictions.txt'\n",
        "\n",
        " return file_voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-V0NUVOh3F3i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DW5st9ADa_-f"
      },
      "outputs": [],
      "source": [
        "def Allmetrics_hard_voting (files):\n",
        " for file in files:\n",
        "  with open(file, 'r') as f:\n",
        "    data = f.read().strip().split('\\n')\n",
        "\n",
        "  sentences_gold_tag = []\n",
        "  sentences_pred_tag = []\n",
        "  gold_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  for line in data[1:]:\n",
        "    if not line.strip():\n",
        "        # Add the current sentence's gold and pred tags to the list of all sentences\n",
        "        sentences_gold_tag.append(gold_tags)\n",
        "        sentences_pred_tag.append(pred_tags)\n",
        "        # Reset the gold_tags and pred_tags lists for the next sentence\n",
        "        gold_tags = []\n",
        "        pred_tags = []\n",
        "    else:\n",
        "        token, gold_tag, pred_tag = line.strip().split('\\t')\n",
        "        gold_tags.append(gold_tag)\n",
        "        pred_tags.append(pred_tag)\n",
        "\n",
        "# Add the gold and pred tags for the last sentence to the list of all sentences\n",
        "  sentences_gold_tag.append(gold_tags)\n",
        "  sentences_pred_tag.append(pred_tags)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "  precision= precision_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  recall= recall_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  f1_score= f1(sentences_gold_tag, sentences_pred_tag, average='micro', scheme=IOB2 )\n",
        "  report = classification_report(sentences_gold_tag, sentences_pred_tag, scheme=IOB2, digits=4)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "  print ('\\n*************\\n', file )\n",
        "\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1_score:.3f}\")\n",
        "\n",
        "  #print (report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi42PbEbHK-M"
      },
      "source": [
        "#Hard Voting for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfzFdZ6w9yyT",
        "outputId": "04f61e85-e261-4766-a1e9-4e712995dce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Allah seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Clan seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Prophet seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Pers seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NatOb seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Loc seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Date seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/Camel_msa_predictions.txt\n",
            "Precision: 0.968\n",
            "Recall: 0.977\n",
            "F1-score: 0.973\n",
            "\n",
            "*************\n",
            " /content/Camel_ca_predictions.txt\n",
            "Precision: 0.975\n",
            "Recall: 0.981\n",
            "F1-score: 0.978\n",
            "\n",
            "*************\n",
            " /content/Camel_mix_predictions.txt\n",
            "Precision: 0.969\n",
            "Recall: 0.978\n",
            "F1-score: 0.974\n",
            "\n",
            "*************\n",
            " /content/multilingual_predictions.txt\n",
            "Precision: 0.961\n",
            "Recall: 0.969\n",
            "F1-score: 0.965\n",
            "\n",
            "*************\n",
            " Compined_Predictions.txt\n",
            "Precision: 0.973\n",
            "Recall: 0.981\n",
            "F1-score: 0.977\n"
          ]
        }
      ],
      "source": [
        "\n",
        "New_files = [\n",
        "          \"/content/Camel_msa_predictions.txt\",\n",
        "       \"/content/Camel_ca_predictions.txt\",\n",
        "         \"/content/Camel_mix_predictions.txt\",\n",
        "      \"/content/multilingual_predictions.txt\",\n",
        "         ]\n",
        "\n",
        "\n",
        "file_vote_predictions = Hard_Voting(New_files)\n",
        "\n",
        "New_files.append(file_vote_predictions)\n",
        "\n",
        "\n",
        "Allmetrics_hard_voting(New_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IQKWUe829al"
      },
      "source": [
        "#Soft Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o5PEW4_YmPZ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l1qtCEndAzI0"
      },
      "outputs": [],
      "source": [
        "def combine_files_for_soft_voting (files):\n",
        " list1 = [[] for h in range(25950)]\n",
        " dic ={}\n",
        "\n",
        "\n",
        " i=0\n",
        " for file , weight in files:\n",
        "    #index = 3\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        k=0\n",
        "        for line in f:\n",
        "\n",
        "            if not line.strip():\n",
        "              list1[k].append([''])\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                token, gold_tag,predicted_tag = line[0], line[1],line[2]\n",
        "                if i==0:\n",
        "                   list1[k].append([token])\n",
        "                   list1[k].append([gold_tag])\n",
        "                   list1[k].append([predicted_tag , weight])\n",
        "\n",
        "                elif any(token in sublist for sublist in list1[k]):\n",
        "                   list1[k].append([predicted_tag , weight])\n",
        "                else:\n",
        "                   dic[token] = [predicted_tag, weight]\n",
        "\n",
        "            k+=1\n",
        "    i+=1\n",
        "\n",
        " for token, tag in dic.items():\n",
        "        for i, sublist in enumerate(list1):\n",
        "          if any(isinstance(sublist, list) and token in sublist for sublist in list1[i]):\n",
        "               #if isinstance(tag, list):\n",
        "               # list1[i].extend(tag)\n",
        "               #else:\n",
        "                list1[i].append(tag)\n",
        "                break\n",
        "\n",
        "\n",
        " with open('Predictions_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for kline in list1:\n",
        "        for item in kline:\n",
        "          if isinstance(item, list):\n",
        "            f.write(','.join(map(str, item)))\n",
        "          else:\n",
        "            f.write(str(item))\n",
        "          f.write('\\t')\n",
        "        f.write('\\n')\n",
        "#    for token , ptags in dic.items():\n",
        " #    f.write( f\"{token} \\t {ptags} \\n\")\n",
        "\n",
        " file_name = '/content/Predictions_soft_hard.txt'\n",
        "\n",
        " return file_name\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kDNkhx7FA398"
      },
      "outputs": [],
      "source": [
        "def Soft_voting(files):\n",
        " model_weight =[]\n",
        " predicted_tags = []\n",
        " vote_counts = {}\n",
        " list2=[]\n",
        " label_sums = {}\n",
        " file_name = combine_files_for_soft_voting(files)\n",
        "\n",
        " with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "   for line in f:\n",
        "\n",
        "            line = line.strip().split()\n",
        "\n",
        "            if len(line)==0:\n",
        "              continue\n",
        "            elif len(line) == 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              pred_tag = line[2]\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "\n",
        "            elif len(line) > 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              for tag in line[2:]:\n",
        "                values = tag.split(',')\n",
        "                if len(values) !=2:\n",
        "                  continue\n",
        "                label , value = values\n",
        "                if label not in vote_counts:\n",
        "                   vote_counts[label] = 0\n",
        "                vote_counts[label] += float(value)\n",
        "              pred_tag = max(vote_counts, key=vote_counts.get)\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "              vote_counts.clear()\n",
        "\n",
        "\n",
        " with open('Compined_Predictions_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for token, gold_tag, pred_tag in list2:\n",
        "        f.write(f\"{token}\\t{gold_tag}\\t{pred_tag}\\n\")\n",
        "\n",
        " file_name = '/content/Compined_Predictions_soft_hard.txt'\n",
        "\n",
        " return file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pptGuW9r15L1"
      },
      "outputs": [],
      "source": [
        "def Allmetrics_soft_voting (files):\n",
        "\n",
        " for file , weight in files:\n",
        "  with open(file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().strip().split('\\n')\n",
        "\n",
        "  sentences_gold_tag = []\n",
        "  sentences_pred_tag = []\n",
        "  gold_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  for line in data[1:]:\n",
        "    if not line.strip():\n",
        "        # Add the current sentence's gold and pred tags to the list of all sentences\n",
        "        sentences_gold_tag.append(gold_tags)\n",
        "        sentences_pred_tag.append(pred_tags)\n",
        "        # Reset the gold_tags and pred_tags lists for the next sentence\n",
        "        gold_tags = []\n",
        "        pred_tags = []\n",
        "    else:\n",
        "        token, gold_tag, pred_tag = line.strip().split('\\t')\n",
        "        gold_tags.append(gold_tag)\n",
        "        pred_tags.append(pred_tag)\n",
        "\n",
        "# Add the gold and pred tags for the last sentence to the list of all sentences\n",
        "  sentences_gold_tag.append(gold_tags)\n",
        "  sentences_pred_tag.append(pred_tags)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "  precision= precision_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  recall= recall_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  f1_score= f1(sentences_gold_tag, sentences_pred_tag, average='micro', scheme=IOB2 )\n",
        "  report = classification_report(sentences_gold_tag, sentences_pred_tag, scheme=IOB2, digits=4)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "  print ('\\n*************\\n', file )\n",
        "\n",
        "  print(f\"Precision: {precision:.4f}\")\n",
        "  print(f\"Recall: {recall:.4f}\")\n",
        "  print(f\"F1-score: {f1_score:.4f}\")\n",
        "\n",
        "  #print (report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbLXt1Q2A8Dr",
        "outputId": "fc2208de-ddb2-4d34-e05e-e528703bb42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Allah seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Clan seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Prophet seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Pers seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NatOb seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Loc seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Date seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/Camel_msa_predictions.txt\n",
            "Precision: 0.9685\n",
            "Recall: 0.9775\n",
            "F1-score: 0.9730\n",
            "\n",
            "*************\n",
            " /content/Camel_ca_predictions.txt\n",
            "Precision: 0.9751\n",
            "Recall: 0.9805\n",
            "F1-score: 0.9778\n",
            "\n",
            "*************\n",
            " /content/Camel_mix_predictions.txt\n",
            "Precision: 0.9694\n",
            "Recall: 0.9784\n",
            "F1-score: 0.9739\n",
            "\n",
            "*************\n",
            " /content/multilingual_predictions.txt\n",
            "Precision: 0.9611\n",
            "Recall: 0.9694\n",
            "F1-score: 0.9653\n",
            "\n",
            "*************\n",
            " /content/Compined_Predictions_soft_hard.txt\n",
            "Precision: 0.9754\n",
            "Recall: 0.9805\n",
            "F1-score: 0.9780\n"
          ]
        }
      ],
      "source": [
        "#weighted for each model\n",
        "\n",
        "files = [\n",
        "         [ \"/content/Camel_msa_predictions.txt\", 0.249487], #929\n",
        "        [\"/content/Camel_ca_predictions.txt\", 0.250769],  #88\n",
        "        [\"/content/Camel_mix_predictions.txt\", 0.249744],  #924\n",
        "        [\"/content/multilingual_predictions.txt\",  0.25]  #936\n",
        "         ]\n",
        "\n",
        "\n",
        "file_vote_predictions = Soft_voting(files)\n",
        "\n",
        "files.append([file_vote_predictions,0.0])\n",
        "\n",
        "\n",
        "Allmetrics_soft_voting(files)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-FpYJV514Zpi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
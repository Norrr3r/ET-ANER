{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsdz6ufUXS1l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW8WVaPBdlyP",
        "outputId": "0ddebc1e-ea52-4b78-daa9-5213428eedc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval==1.2.2 in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from seqeval==1.2.2) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval==1.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnhNVHAjCAnt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DW6OGXUdquL"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score as f1,\n",
        "    accuracy_score)\n",
        "from seqeval.scheme import IOB2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4xtz25GJZ2K"
      },
      "outputs": [],
      "source": [
        "def combine_predictions_for_hard_voting (files):\n",
        " #out = open(\"out.txt\", \"r+\")\n",
        " bst='\\t'\n",
        " list1 = [[] for h in range(25950)]\n",
        " dic ={}\n",
        "\n",
        "\n",
        " i=0  #index for files\n",
        " for file in files:\n",
        "    #index = 3\n",
        "    with open( file, 'r', encoding='utf-8') as f:\n",
        "        k=0   #index for lines\n",
        "        for line in f:\n",
        "\n",
        "            if not line.strip():\n",
        "              list1[k].append([''])\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                token, gold_tag,predicted_tag = line[0], line[1],line[2]\n",
        "                if i==0:\n",
        "                   list1[k].append([token])\n",
        "                   list1[k].append([gold_tag])\n",
        "                   list1[k].append([predicted_tag])\n",
        "                #else:\n",
        "                 #   list1[k].append([predicted_tag])\n",
        "\n",
        "                elif any(token in sublist for sublist in list1[k]):\n",
        "                   list1[k].append([predicted_tag])\n",
        "                else:\n",
        "                   dic[token] = predicted_tag\n",
        "\n",
        "            k+=1\n",
        "    i+=1\n",
        "\n",
        " for token, tag in dic.items():\n",
        "       for i, sublist in enumerate(list1):\n",
        "            if any(token in sublist for sublist in list1[i]):\n",
        "               list1[i].append([tag])\n",
        "               break\n",
        "\n",
        "\n",
        " with open('/content/Predictions_test.txt', 'w', encoding='utf-8') as f:\n",
        "    for kline in list1:\n",
        "        for item in kline:\n",
        "            f.write( bst.join(item) + '\\t')\n",
        "        f.write('\\n')\n",
        "#    for token , ptags in dic.items():\n",
        " #    f.write( f\"{token} \\t {ptags} \\n\")\n",
        "\n",
        " file_name = '/content/Predictions_test.txt'\n",
        " return file_name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM13iwzRKIhY"
      },
      "outputs": [],
      "source": [
        "def Hard_Voting (files):\n",
        " predicted_tags = []\n",
        " vote_counts = {}\n",
        " list2=[]\n",
        " file_name = combine_predictions_for_hard_voting(files)\n",
        " with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "   for line in f:\n",
        "            line = line.strip().split()\n",
        "\n",
        "            if len(line)==0:\n",
        "              continue\n",
        "\n",
        "            elif len(line)==3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              pred_tag = line[2]\n",
        "              list2.append((token ,gold_tag, pred_tag))\n",
        "\n",
        "            elif len(line) >3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "\n",
        "              for tag in line[2:]:\n",
        "                if tag not in vote_counts:\n",
        "                   vote_counts[tag] = 0\n",
        "                vote_counts[tag] += 1\n",
        "              pred_tag = max(vote_counts, key=vote_counts.get)\n",
        "              list2.append((token ,gold_tag, pred_tag))\n",
        "              vote_counts.clear()\n",
        "\n",
        "\n",
        " with open('Compined_Predictions.txt', 'w', encoding='utf-8') as f:\n",
        "    for token, gold_tag, pred_tag in list2:\n",
        "        f.write(f\"{token}\\t{gold_tag}\\t{pred_tag}\\n\")\n",
        "\n",
        " file_voting = 'Compined_Predictions.txt'\n",
        "\n",
        " return file_voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V0NUVOh3F3i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW5st9ADa_-f"
      },
      "outputs": [],
      "source": [
        "def Allmetrics_hard_voting (files):\n",
        " for file in files:\n",
        "  with open(file, 'r') as f:\n",
        "    data = f.read().strip().split('\\n')\n",
        "\n",
        "  sentences_gold_tag = []\n",
        "  sentences_pred_tag = []\n",
        "  gold_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  for line in data[1:]:\n",
        "    if not line.strip():\n",
        "        # Add the current sentence's gold and pred tags to the list of all sentences\n",
        "        sentences_gold_tag.append(gold_tags)\n",
        "        sentences_pred_tag.append(pred_tags)\n",
        "        # Reset the gold_tags and pred_tags lists for the next sentence\n",
        "        gold_tags = []\n",
        "        pred_tags = []\n",
        "    else:\n",
        "        token, gold_tag, pred_tag = line.strip().split('\\t')\n",
        "        gold_tags.append(gold_tag)\n",
        "        pred_tags.append(pred_tag)\n",
        "\n",
        "# Add the gold and pred tags for the last sentence to the list of all sentences\n",
        "  sentences_gold_tag.append(gold_tags)\n",
        "  sentences_pred_tag.append(pred_tags)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "  precision= precision_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  recall= recall_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  f1_score= f1(sentences_gold_tag, sentences_pred_tag, average='micro', scheme=IOB2 )\n",
        "  report = classification_report(sentences_gold_tag, sentences_pred_tag, scheme=IOB2, digits=4)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "  print ('\\n*************\\n', file )\n",
        "\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1_score:.3f}\")\n",
        "\n",
        "  #print (report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi42PbEbHK-M"
      },
      "source": [
        "#Hard Voting for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfzFdZ6w9yyT",
        "outputId": "21ef628d-cebe-4b28-c57c-05b920308bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.942\n",
            "F1-score: 0.929\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "Precision: 0.872\n",
            "Recall: 0.885\n",
            "F1-score: 0.879\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.932\n",
            "F1-score: 0.924\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "Precision: 0.923\n",
            "Recall: 0.950\n",
            "F1-score: 0.936\n",
            "\n",
            "*************\n",
            " Compined_Predictions.txt\n",
            "Precision: 0.928\n",
            "Recall: 0.946\n",
            "F1-score: 0.937\n"
          ]
        }
      ],
      "source": [
        "\n",
        "New_files = [ #\"/content/predictions_ANERtestC10_Arbert.txt\",\n",
        "             # \"/content/predictions_ANERtestC10_Arabert.txt\",\n",
        "        # \"/content/predictions_ANERtestC10_MARBERT.txt\",\n",
        "     # \"/content/predictions_ANERtestC10_Arabicner.txt\",\n",
        "     #  \"/content/predictions_ANERtestC10_Qarib.txt\",\n",
        "     #  \"/content/predictions_ANERtestC10_Asafya.txt\",\n",
        "     #  \"/content/predictions_ANERtestC10_hatimimoh.txt\",\n",
        "          \"/content/predictions_ANERtestC10_Camel-msa-ner.txt\",\n",
        "       \"/content/predictions_ANERtestC10_Camel-ca-ner.txt\",\n",
        "         \"/content/predictions_ANERtestC10_Camel-mix-ner.txt\",\n",
        "      \"/content/predictions_ANERtestC10_Multilingual2.txt\",\n",
        "         ]\n",
        "\n",
        "\n",
        "file_vote_predictions = Hard_Voting(New_files)\n",
        "\n",
        "New_files.append(file_vote_predictions)\n",
        "\n",
        "\n",
        "Allmetrics_hard_voting(New_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IQKWUe829al"
      },
      "source": [
        "#Soft Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5PEW4_YmPZ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1qtCEndAzI0"
      },
      "outputs": [],
      "source": [
        "def combine_files_for_soft_voting (files):\n",
        " list1 = [[] for h in range(25950)]\n",
        " dic ={}\n",
        "\n",
        "\n",
        " i=0\n",
        " for file , weight in files:\n",
        "    #index = 3\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        k=0\n",
        "        for line in f:\n",
        "\n",
        "            if not line.strip():\n",
        "              list1[k].append([''])\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                token, gold_tag,predicted_tag = line[0], line[1],line[2]\n",
        "                if i==0:\n",
        "                   list1[k].append([token])\n",
        "                   list1[k].append([gold_tag])\n",
        "                   list1[k].append([predicted_tag , weight])\n",
        "\n",
        "                elif any(token in sublist for sublist in list1[k]):\n",
        "                   list1[k].append([predicted_tag , weight])\n",
        "                else:\n",
        "                   dic[token] = [predicted_tag, weight]\n",
        "\n",
        "            k+=1\n",
        "    i+=1\n",
        "\n",
        " for token, tag in dic.items():\n",
        "        for i, sublist in enumerate(list1):\n",
        "          if any(isinstance(sublist, list) and token in sublist for sublist in list1[i]):\n",
        "               #if isinstance(tag, list):\n",
        "               # list1[i].extend(tag)\n",
        "               #else:\n",
        "                list1[i].append(tag)\n",
        "                break\n",
        "\n",
        "\n",
        " with open('Predictions_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for kline in list1:\n",
        "        for item in kline:\n",
        "          if isinstance(item, list):\n",
        "            f.write(','.join(map(str, item)))\n",
        "          else:\n",
        "            f.write(str(item))\n",
        "          f.write('\\t')\n",
        "        f.write('\\n')\n",
        "#    for token , ptags in dic.items():\n",
        " #    f.write( f\"{token} \\t {ptags} \\n\")\n",
        "\n",
        " file_name = '/content/Predictions_soft_hard.txt'\n",
        "\n",
        " return file_name\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDNkhx7FA398"
      },
      "outputs": [],
      "source": [
        "def Soft_voting(files):\n",
        " model_weight =[]\n",
        " predicted_tags = []\n",
        " vote_counts = {}\n",
        " list2=[]\n",
        " label_sums = {}\n",
        " file_name = combine_files_for_soft_voting(files)\n",
        "\n",
        " with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "   for line in f:\n",
        "\n",
        "            line = line.strip().split()\n",
        "\n",
        "            if len(line)==0:\n",
        "              continue\n",
        "            elif len(line) == 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              pred_tag = line[2]\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "\n",
        "            elif len(line) > 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              for tag in line[2:]:\n",
        "                values = tag.split(',')\n",
        "                if len(values) !=2:\n",
        "                  continue\n",
        "                label , value = values\n",
        "                if label not in vote_counts:\n",
        "                   vote_counts[label] = 0\n",
        "                vote_counts[label] += float(value)\n",
        "              pred_tag = max(vote_counts, key=vote_counts.get)\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "              vote_counts.clear()\n",
        "\n",
        "\n",
        " with open('Compined_Predictions_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for token, gold_tag, pred_tag in list2:\n",
        "        f.write(f\"{token}\\t{gold_tag}\\t{pred_tag}\\n\")\n",
        "\n",
        " file_name = '/content/Compined_Predictions_soft_hard.txt'\n",
        "\n",
        " return file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pptGuW9r15L1"
      },
      "outputs": [],
      "source": [
        "def Allmetrics_soft_voting (files):\n",
        "\n",
        " for file , weight in files:\n",
        "  with open(file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().strip().split('\\n')\n",
        "\n",
        "  sentences_gold_tag = []\n",
        "  sentences_pred_tag = []\n",
        "  gold_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  for line in data[1:]:\n",
        "    if not line.strip():\n",
        "        # Add the current sentence's gold and pred tags to the list of all sentences\n",
        "        sentences_gold_tag.append(gold_tags)\n",
        "        sentences_pred_tag.append(pred_tags)\n",
        "        # Reset the gold_tags and pred_tags lists for the next sentence\n",
        "        gold_tags = []\n",
        "        pred_tags = []\n",
        "    else:\n",
        "        token, gold_tag, pred_tag = line.strip().split('\\t')\n",
        "        gold_tags.append(gold_tag)\n",
        "        pred_tags.append(pred_tag)\n",
        "\n",
        "# Add the gold and pred tags for the last sentence to the list of all sentences\n",
        "  sentences_gold_tag.append(gold_tags)\n",
        "  sentences_pred_tag.append(pred_tags)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "  precision= precision_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  recall= recall_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  f1_score= f1(sentences_gold_tag, sentences_pred_tag, average='micro', scheme=IOB2 )\n",
        "  report = classification_report(sentences_gold_tag, sentences_pred_tag, scheme=IOB2, digits=4)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "  print ('\\n*************\\n', file )\n",
        "\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1_score:.3f}\")\n",
        "\n",
        "  #print (report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbLXt1Q2A8Dr",
        "outputId": "37c98833-2045-4463-d546-438a4eac9879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.942\n",
            "F1-score: 0.929\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "Precision: 0.872\n",
            "Recall: 0.885\n",
            "F1-score: 0.879\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.932\n",
            "F1-score: 0.924\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "Precision: 0.923\n",
            "Recall: 0.950\n",
            "F1-score: 0.936\n",
            "\n",
            "*************\n",
            " /content/Compined_Predictions_soft_hard.txt\n",
            "Precision: 0.938\n",
            "Recall: 0.954\n",
            "F1-score: 0.946\n"
          ]
        }
      ],
      "source": [
        "#weighted for each model\n",
        "\n",
        "files = [ #[\"/content/predictions_ANERtestC10_Arbert.txt\",0.0],   #87%\n",
        "           #  [\"/content/predictions_ANERtestC10_Arabert.txt\",0.0],   #82\n",
        "           #[\"/content/predictions_ANERtestC10_MARBERT.txt\",0.0],     #84\n",
        "         #[\"/content/predictions_ANERtestC10_Arabicner.txt\",0.0],   #86\n",
        "         #[\"/content/predictions_ANERtestC10_Qarib.txt\",0.0],      #84\n",
        "         #[\"/content/predictions_ANERtestC10_Asafya.txt\",0.0],    #84\n",
        "         #[\"/content/predictions_ANERtestC10_hatimimoh.txt\",0.0],    #83\n",
        "         [\"/content/predictions_ANERtestC10_Camel-msa-ner.txt\", 0.30], #929\n",
        "        [\"/content/predictions_ANERtestC10_Camel-ca-ner.txt\", 0.16],  #88\n",
        "        [\"/content/predictions_ANERtestC10_Camel-mix-ner.txt\", 0.18],  #924\n",
        "        [\"/content/predictions_ANERtestC10_Multilingual2.txt\",  0.36]  #936\n",
        "         ]\n",
        "\n",
        "\n",
        "\n",
        "file_vote_predictions = Soft_voting(files)\n",
        "\n",
        "files.append([file_vote_predictions,0.0])\n",
        "\n",
        "\n",
        "Allmetrics_soft_voting(files)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-FpYJV514Zpi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JrbM28fEb1j"
      },
      "source": [
        "#New Code for Weighted voting (this code is ony for testing)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def class_probabilities (file_name , weight , model_number ):\n",
        "  # Initialize a dictionary to store the counts of each class\n",
        " class_counts = {'O': 0, 'B-PERS': 0, 'I-PERS': 0, 'I-ORG': 0, 'B-ORG': 0, 'I-LOC': 0, 'B-LOC': 0, 'I-MISC': 0, 'B-MISC': 0}\n",
        " total_lines = 0\n",
        " Model_probabilities = []\n",
        " new_file_lines =[]\n",
        "# Read the file\n",
        " #for file , weight in files:\n",
        " with open(file_name, 'r') as f:\n",
        "    for line in f:\n",
        "        total_lines += 1\n",
        "        line = line.strip().split()\n",
        "        if len(line)==0:\n",
        "          continue\n",
        "        elif len(line) ==3:\n",
        "          pred_tag = line[2]\n",
        "          class_counts[pred_tag] += 1\n",
        "\n",
        " class_probabilities = {class_name: (count / total_lines)*weight for class_name, count in class_counts.items()}\n",
        " Model_probabilities.append(class_probabilities)\n",
        "# Print the probabilities\n",
        " #for i, model_probs in enumerate(Model_probabilities):\n",
        " #   print(f\"Model {i+1} probabilities:\")\n",
        " #   for class_name, probability in model_probs.items():\n",
        " #       print(f\"Class: {class_name}\\tProbability: {probability}\")\n",
        "  #  print()\n",
        " probabilities_file_name =  f'probabilities_{model_number}.txt'\n",
        " with open(probabilities_file_name, 'w') as new_file:\n",
        "    for model_probs in Model_probabilities:\n",
        "        with open(file_name, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip().split()\n",
        "                if len(line) == 3:\n",
        "                    token, gold_tag, pred_tag = line\n",
        "                    probability = model_probs[pred_tag]\n",
        "                    new_line = f\"{token}\\t{gold_tag}\\t{pred_tag},{probability}\"\n",
        "                    new_file.write(new_line + \"\\n\")\n",
        "                    new_file_lines.append(new_line)\n",
        "                else:\n",
        "                    new_file.write(\" \".join(line) + \"\\n\")\n",
        "                    new_file_lines.append(\" \".join(line))\n",
        "'''"
      ],
      "metadata": {
        "id": "rFY0GyCenI44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def create_files(files):\n",
        "  model_no=1\n",
        "  list1=[]\n",
        "  for file_name , weight in files:\n",
        "    class_probabilities(file_name , weight , model_no)\n",
        "    list1.append(\"probabilities_\"+ str(model_no)+\".txt\")\n",
        "    model_no+=1\n",
        "  return list1\n",
        "  '''"
      ],
      "metadata": {
        "id": "jDTL-hSM1BqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeZF4Kgc1AAg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def New_combine_files_for_soft_voting (files):\n",
        " list1 = [[] for h in range(15468)]\n",
        " dic ={}\n",
        " l=[]\n",
        " l=create_files(files)\n",
        " i=0\n",
        " for file in l:\n",
        "    #index = 3\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        k=0\n",
        "        for line in f:\n",
        "\n",
        "            if not line.strip():\n",
        "              list1[k].append([''])\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                token, gold_tag,predicted_tag = line[0], line[1],line[2]\n",
        "                if i==0:\n",
        "                   list1[k].append([token])\n",
        "                   list1[k].append([gold_tag])\n",
        "                   list1[k].append([predicted_tag])\n",
        "\n",
        "                elif any(token in sublist for sublist in list1[k]):\n",
        "                   list1[k].append([predicted_tag])\n",
        "                else:\n",
        "                   dic[token] = [predicted_tag]\n",
        "\n",
        "            k+=1\n",
        "    i+=1\n",
        "\n",
        " for token, tag in dic.items():\n",
        "        for i, sublist in enumerate(list1):\n",
        "          if any(isinstance(sublist, list) and token in sublist for sublist in list1[i]):\n",
        "               #if isinstance(tag, list):\n",
        "               # list1[i].extend(tag)\n",
        "               #else:\n",
        "                list1[i].append(tag)\n",
        "                break\n",
        "\n",
        "\n",
        " with open('Predictions_New_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for kline in list1:\n",
        "        for item in kline:\n",
        "          if isinstance(item, list):\n",
        "            f.write(','.join(map(str, item)))\n",
        "          else:\n",
        "            f.write(str(item))\n",
        "          f.write('\\t')\n",
        "        f.write('\\n')\n",
        "#    for token , ptags in dic.items():\n",
        " #    f.write( f\"{token} \\t {ptags} \\n\")\n",
        "\n",
        " file_name = '/content/Predictions_New_soft_hard.txt'\n",
        "\n",
        " return file_name\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def New_Soft_voting(files):\n",
        " model_weight =[]\n",
        " predicted_tags = []\n",
        " vote_counts = {}\n",
        " list2=[]\n",
        " label_sums = {}\n",
        " file_name = New_combine_files_for_soft_voting(files)\n",
        "\n",
        " with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "   for line in f:\n",
        "\n",
        "            line = line.strip().split('\\t')\n",
        "\n",
        "            if len(line)==0:\n",
        "              continue\n",
        "            elif len(line) == 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              preb_tag , prob = line[2].split(',')\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "\n",
        "            elif len(line) > 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              for tag in line[2:]:\n",
        "                values = tag.split(',')\n",
        "                if len(values) !=2:\n",
        "                  continue\n",
        "                label , value = values\n",
        "                if label not in vote_counts:\n",
        "                   vote_counts[label] = 0\n",
        "                vote_counts[label] += float(value)\n",
        "              if not vote_counts:\n",
        "               continue\n",
        "              else:\n",
        "                pred_tag = max(vote_counts, key=vote_counts.get)\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "              vote_counts.clear()\n",
        "\n",
        " with open('New_Compined_Predictions_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for token, gold_tag, pred_tag in list2:\n",
        "        f.write(f\"{token}\\t{gold_tag}\\t{pred_tag}\\n\")\n",
        "\n",
        " file_name = '/content/New_Compined_Predictions_soft_hard.txt'\n",
        "\n",
        " return file_name\n",
        " '''"
      ],
      "metadata": {
        "id": "2-9ba5KbmXIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def New_Allmetrics_soft_voting (files):\n",
        "\n",
        " for file , weight in files:\n",
        "  with open(file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().strip().split('\\n')\n",
        "\n",
        "  sentences_gold_tag = []\n",
        "  sentences_pred_tag = []\n",
        "  gold_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  for line in data[1:]:\n",
        "    if not line.strip():\n",
        "        # Add the current sentence's gold and pred tags to the list of all sentences\n",
        "        sentences_gold_tag.append(gold_tags)\n",
        "        sentences_pred_tag.append(pred_tags)\n",
        "        # Reset the gold_tags and pred_tags lists for the next sentence\n",
        "        gold_tags = []\n",
        "        pred_tags = []\n",
        "    else:\n",
        "        token, gold_tag, pred_tag = line.strip().split('\\t')\n",
        "        gold_tags.append(gold_tag)\n",
        "        pred_tags.append(pred_tag)\n",
        "\n",
        "# Add the gold and pred tags for the last sentence to the list of all sentences\n",
        "  sentences_gold_tag.append(gold_tags)\n",
        "  sentences_pred_tag.append(pred_tags)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "  precision= precision_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  recall= recall_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  f1_score= f1(sentences_gold_tag, sentences_pred_tag, average='micro', scheme=IOB2 )\n",
        "  report = classification_report(sentences_gold_tag, sentences_pred_tag, scheme=IOB2, digits=4)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "  print ('\\n*************\\n', file )\n",
        "\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1_score:.3f}\")\n",
        "\n",
        "  #print (report)\n",
        "  '''"
      ],
      "metadata": {
        "id": "paRczvVxmdWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted for each model\n",
        "'''\n",
        "files = [ #[\"/content/predictions_ANERtestC10_Arbert.txt\",0.0],   #87%\n",
        "           #  [\"/content/predictions_ANERtestC10_Arabert.txt\",0.0],   #82\n",
        "           #[\"/content/predictions_ANERtestC10_MARBERT.txt\",0.0],     #84\n",
        "         #[\"/content/predictions_ANERtestC10_Arabicner.txt\",0.0],   #86\n",
        "         #[\"/content/predictions_ANERtestC10_Qarib.txt\",0.0],      #84\n",
        "         #[\"/content/predictions_ANERtestC10_Asafya.txt\",0.0],    #84\n",
        "         #[\"/content/predictions_ANERtestC10_hatimimoh.txt\",0.0],    #83\n",
        "         [\"/content/predictions_ANERtestC10_Camel-msa-ner.txt\", 0.30], #929\n",
        "        [\"/content/predictions_ANERtestC10_Camel-ca-ner.txt\", 0.16],  #88\n",
        "        [\"/content/predictions_ANERtestC10_Camel-mix-ner.txt\", 0.18],  #924\n",
        "        [\"/content/predictions_ANERtestC10_Multilingual.txt\",  0.36]  #936\n",
        "         ]\n",
        "\n",
        "\n",
        "\n",
        "file_vote_predictions = New_Soft_voting(files)\n",
        "\n",
        "files.append([file_vote_predictions,0.0])\n",
        "\n",
        "\n",
        "Allmetrics_soft_voting(files)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99pLArBumlEM",
        "outputId": "e89c5a53-780d-41d1-face-8ef757a02adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.942\n",
            "F1-score: 0.929\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "Precision: 0.872\n",
            "Recall: 0.885\n",
            "F1-score: 0.879\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.932\n",
            "F1-score: 0.924\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "Precision: 0.923\n",
            "Recall: 0.950\n",
            "F1-score: 0.936\n",
            "\n",
            "*************\n",
            " /content/New_Compined_Predictions_soft_hard.txt\n",
            "Precision: 0.954\n",
            "Recall: 0.919\n",
            "F1-score: 0.936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHD8W-_zS2lW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
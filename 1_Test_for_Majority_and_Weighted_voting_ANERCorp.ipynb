{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Norrr3r/ET-ANER/blob/main/1_Test_for_Majority_and_Weighted_voting_ANERCorp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fsdz6ufUXS1l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW8WVaPBdlyP",
        "outputId": "ad99433e-27cd-4444-ce9b-c0375474be2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval==1.2.2 in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from seqeval==1.2.2) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval==1.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JnhNVHAjCAnt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8DW6OGXUdquL"
      },
      "outputs": [],
      "source": [
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score as f1,\n",
        "    accuracy_score)\n",
        "from seqeval.scheme import IOB2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l4xtz25GJZ2K"
      },
      "outputs": [],
      "source": [
        "def combine_predictions_for_hard_voting (files):\n",
        " #out = open(\"out.txt\", \"r+\")\n",
        " bst='\\t'\n",
        " list1 = [[] for h in range(25950)]\n",
        " dic ={}\n",
        "\n",
        "\n",
        " i=0  #index for files\n",
        " for file in files:\n",
        "    #index = 3\n",
        "    with open( file, 'r', encoding='utf-8') as f:\n",
        "        k=0   #index for lines\n",
        "        for line in f:\n",
        "\n",
        "            if not line.strip():\n",
        "              list1[k].append([''])\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                token, gold_tag,predicted_tag = line[0], line[1],line[2]\n",
        "                if i==0:\n",
        "                   list1[k].append([token])\n",
        "                   list1[k].append([gold_tag])\n",
        "                   list1[k].append([predicted_tag])\n",
        "                #else:\n",
        "                 #   list1[k].append([predicted_tag])\n",
        "\n",
        "                elif any(token in sublist for sublist in list1[k]):\n",
        "                   list1[k].append([predicted_tag])\n",
        "                else:\n",
        "                   dic[token] = predicted_tag\n",
        "\n",
        "            k+=1\n",
        "    i+=1\n",
        "\n",
        " for token, tag in dic.items():\n",
        "       for i, sublist in enumerate(list1):\n",
        "            if any(token in sublist for sublist in list1[i]):\n",
        "               list1[i].append([tag])\n",
        "               break\n",
        "\n",
        "\n",
        " with open('/content/Predictions_test.txt', 'w', encoding='utf-8') as f:\n",
        "    for kline in list1:\n",
        "        for item in kline:\n",
        "            f.write( bst.join(item) + '\\t')\n",
        "        f.write('\\n')\n",
        "#    for token , ptags in dic.items():\n",
        " #    f.write( f\"{token} \\t {ptags} \\n\")\n",
        "\n",
        " file_name = '/content/Predictions_test.txt'\n",
        " return file_name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yM13iwzRKIhY"
      },
      "outputs": [],
      "source": [
        "def Hard_Voting (files):\n",
        " predicted_tags = []\n",
        " vote_counts = {}\n",
        " list2=[]\n",
        " file_name = combine_predictions_for_hard_voting(files)\n",
        " with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "   for line in f:\n",
        "            line = line.strip().split()\n",
        "\n",
        "            if len(line)==0:\n",
        "              continue\n",
        "\n",
        "            elif len(line)==3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              pred_tag = line[2]\n",
        "              list2.append((token ,gold_tag, pred_tag))\n",
        "\n",
        "            elif len(line) >3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "\n",
        "              for tag in line[2:]:\n",
        "                if tag not in vote_counts:\n",
        "                   vote_counts[tag] = 0\n",
        "                vote_counts[tag] += 1\n",
        "              pred_tag = max(vote_counts, key=vote_counts.get)\n",
        "              list2.append((token ,gold_tag, pred_tag))\n",
        "              vote_counts.clear()\n",
        "\n",
        "\n",
        " with open('Compined_Predictions.txt', 'w', encoding='utf-8') as f:\n",
        "    for token, gold_tag, pred_tag in list2:\n",
        "        f.write(f\"{token}\\t{gold_tag}\\t{pred_tag}\\n\")\n",
        "\n",
        " file_voting = 'Compined_Predictions.txt'\n",
        "\n",
        " return file_voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-V0NUVOh3F3i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DW5st9ADa_-f"
      },
      "outputs": [],
      "source": [
        "def Allmetrics_hard_voting (files):\n",
        " for file in files:\n",
        "  with open(file, 'r') as f:\n",
        "    data = f.read().strip().split('\\n')\n",
        "\n",
        "  sentences_gold_tag = []\n",
        "  sentences_pred_tag = []\n",
        "  gold_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  for line in data[1:]:\n",
        "    if not line.strip():\n",
        "        # Add the current sentence's gold and pred tags to the list of all sentences\n",
        "        sentences_gold_tag.append(gold_tags)\n",
        "        sentences_pred_tag.append(pred_tags)\n",
        "        # Reset the gold_tags and pred_tags lists for the next sentence\n",
        "        gold_tags = []\n",
        "        pred_tags = []\n",
        "    else:\n",
        "        token, gold_tag, pred_tag = line.strip().split('\\t')\n",
        "        gold_tags.append(gold_tag)\n",
        "        pred_tags.append(pred_tag)\n",
        "\n",
        "# Add the gold and pred tags for the last sentence to the list of all sentences\n",
        "  sentences_gold_tag.append(gold_tags)\n",
        "  sentences_pred_tag.append(pred_tags)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "  precision= precision_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  recall= recall_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  f1_score= f1(sentences_gold_tag, sentences_pred_tag, average='micro', scheme=IOB2 )\n",
        "  report = classification_report(sentences_gold_tag, sentences_pred_tag, scheme=IOB2, digits=4)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "  print ('\\n*************\\n', file )\n",
        "\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1_score:.3f}\")\n",
        "\n",
        "  #print (report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi42PbEbHK-M"
      },
      "source": [
        "#Hard Voting for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfzFdZ6w9yyT",
        "outputId": "6217cbf7-4aea-4556-b17a-003be5171f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.942\n",
            "F1-score: 0.929\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "Precision: 0.872\n",
            "Recall: 0.885\n",
            "F1-score: 0.879\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.932\n",
            "F1-score: 0.924\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "Precision: 0.923\n",
            "Recall: 0.950\n",
            "F1-score: 0.936\n",
            "\n",
            "*************\n",
            " Compined_Predictions.txt\n",
            "Precision: 0.928\n",
            "Recall: 0.946\n",
            "F1-score: 0.937\n"
          ]
        }
      ],
      "source": [
        "\n",
        "New_files = [ #\"/content/predictions_ANERtestC10_Arbert.txt\",\n",
        "             # \"/content/predictions_ANERtestC10_Arabert.txt\",\n",
        "        # \"/content/predictions_ANERtestC10_MARBERT.txt\",\n",
        "     # \"/content/predictions_ANERtestC10_Arabicner.txt\",\n",
        "     #  \"/content/predictions_ANERtestC10_Qarib.txt\",\n",
        "     #  \"/content/predictions_ANERtestC10_Asafya.txt\",\n",
        "     #  \"/content/predictions_ANERtestC10_hatimimoh.txt\",\n",
        "          \"/content/predictions_ANERtestC10_Camel-msa-ner.txt\",\n",
        "       \"/content/predictions_ANERtestC10_Camel-ca-ner.txt\",\n",
        "         \"/content/predictions_ANERtestC10_Camel-mix-ner.txt\",\n",
        "      \"/content/predictions_ANERtestC10_Multilingual2.txt\",\n",
        "         ]\n",
        "\n",
        "\n",
        "file_vote_predictions = Hard_Voting(New_files)\n",
        "\n",
        "New_files.append(file_vote_predictions)\n",
        "\n",
        "\n",
        "Allmetrics_hard_voting(New_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IQKWUe829al"
      },
      "source": [
        "#Soft Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "o5PEW4_YmPZ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l1qtCEndAzI0"
      },
      "outputs": [],
      "source": [
        "def combine_files_for_soft_voting (files):\n",
        " list1 = [[] for h in range(25950)]\n",
        " dic ={}\n",
        "\n",
        "\n",
        " i=0\n",
        " for file , weight in files:\n",
        "    #index = 3\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        k=0\n",
        "        for line in f:\n",
        "\n",
        "            if not line.strip():\n",
        "              list1[k].append([''])\n",
        "\n",
        "            else:\n",
        "                line = line.strip().split()\n",
        "                token, gold_tag,predicted_tag = line[0], line[1],line[2]\n",
        "                if i==0:\n",
        "                   list1[k].append([token])\n",
        "                   list1[k].append([gold_tag])\n",
        "                   list1[k].append([predicted_tag , weight])\n",
        "\n",
        "                elif any(token in sublist for sublist in list1[k]):\n",
        "                   list1[k].append([predicted_tag , weight])\n",
        "                else:\n",
        "                   dic[token] = [predicted_tag, weight]\n",
        "\n",
        "            k+=1\n",
        "    i+=1\n",
        "\n",
        " for token, tag in dic.items():\n",
        "        for i, sublist in enumerate(list1):\n",
        "          if any(isinstance(sublist, list) and token in sublist for sublist in list1[i]):\n",
        "               #if isinstance(tag, list):\n",
        "               # list1[i].extend(tag)\n",
        "               #else:\n",
        "                list1[i].append(tag)\n",
        "                break\n",
        "\n",
        "\n",
        " with open('Predictions_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for kline in list1:\n",
        "        for item in kline:\n",
        "          if isinstance(item, list):\n",
        "            f.write(','.join(map(str, item)))\n",
        "          else:\n",
        "            f.write(str(item))\n",
        "          f.write('\\t')\n",
        "        f.write('\\n')\n",
        "#    for token , ptags in dic.items():\n",
        " #    f.write( f\"{token} \\t {ptags} \\n\")\n",
        "\n",
        " file_name = '/content/Predictions_soft_hard.txt'\n",
        "\n",
        " return file_name\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kDNkhx7FA398"
      },
      "outputs": [],
      "source": [
        "def Soft_voting(files):\n",
        " model_weight =[]\n",
        " predicted_tags = []\n",
        " vote_counts = {}\n",
        " list2=[]\n",
        " label_sums = {}\n",
        " file_name = combine_files_for_soft_voting(files)\n",
        "\n",
        " with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "   for line in f:\n",
        "\n",
        "            line = line.strip().split()\n",
        "\n",
        "            if len(line)==0:\n",
        "              continue\n",
        "            elif len(line) == 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              pred_tag = line[2]\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "\n",
        "            elif len(line) > 3:\n",
        "              token = line[0]\n",
        "              gold_tag = line[1]\n",
        "              for tag in line[2:]:\n",
        "                values = tag.split(',')\n",
        "                if len(values) !=2:\n",
        "                  continue\n",
        "                label , value = values\n",
        "                if label not in vote_counts:\n",
        "                   vote_counts[label] = 0\n",
        "                vote_counts[label] += float(value)\n",
        "              pred_tag = max(vote_counts, key=vote_counts.get)\n",
        "              list2.append((token , gold_tag, pred_tag))\n",
        "              vote_counts.clear()\n",
        "\n",
        "\n",
        " with open('Compined_Predictions_soft_hard.txt', 'w', encoding='utf-8') as f:\n",
        "    for token, gold_tag, pred_tag in list2:\n",
        "        f.write(f\"{token}\\t{gold_tag}\\t{pred_tag}\\n\")\n",
        "\n",
        " file_name = '/content/Compined_Predictions_soft_hard.txt'\n",
        "\n",
        " return file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pptGuW9r15L1"
      },
      "outputs": [],
      "source": [
        "def Allmetrics_soft_voting (files):\n",
        "\n",
        " for file , weight in files:\n",
        "  with open(file, 'r', encoding='utf-8') as f:\n",
        "    data = f.read().strip().split('\\n')\n",
        "\n",
        "  sentences_gold_tag = []\n",
        "  sentences_pred_tag = []\n",
        "  gold_tags = []\n",
        "  pred_tags = []\n",
        "\n",
        "  for line in data[1:]:\n",
        "    if not line.strip():\n",
        "        # Add the current sentence's gold and pred tags to the list of all sentences\n",
        "        sentences_gold_tag.append(gold_tags)\n",
        "        sentences_pred_tag.append(pred_tags)\n",
        "        # Reset the gold_tags and pred_tags lists for the next sentence\n",
        "        gold_tags = []\n",
        "        pred_tags = []\n",
        "    else:\n",
        "        token, gold_tag, pred_tag = line.strip().split('\\t')\n",
        "        gold_tags.append(gold_tag)\n",
        "        pred_tags.append(pred_tag)\n",
        "\n",
        "# Add the gold and pred tags for the last sentence to the list of all sentences\n",
        "  sentences_gold_tag.append(gold_tags)\n",
        "  sentences_pred_tag.append(pred_tags)\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "  precision= precision_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  recall= recall_score(sentences_gold_tag, sentences_pred_tag, average='micro' , scheme=IOB2)\n",
        "  f1_score= f1(sentences_gold_tag, sentences_pred_tag, average='micro', scheme=IOB2 )\n",
        "  report = classification_report(sentences_gold_tag, sentences_pred_tag, scheme=IOB2, digits=4)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "  print ('\\n*************\\n', file )\n",
        "\n",
        "  print(f\"Precision: {precision:.3f}\")\n",
        "  print(f\"Recall: {recall:.3f}\")\n",
        "  print(f\"F1-score: {f1_score:.3f}\")\n",
        "\n",
        "  #print (report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbLXt1Q2A8Dr",
        "outputId": "1734909b-3402-4666-d4d9-54cc4e61d918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.942\n",
            "F1-score: 0.929\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "Precision: 0.872\n",
            "Recall: 0.885\n",
            "F1-score: 0.879\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.932\n",
            "F1-score: 0.924\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "Precision: 0.923\n",
            "Recall: 0.950\n",
            "F1-score: 0.936\n",
            "\n",
            "*************\n",
            " /content/Compined_Predictions_soft_hard.txt\n",
            "Precision: 0.938\n",
            "Recall: 0.954\n",
            "F1-score: 0.946\n"
          ]
        }
      ],
      "source": [
        "#weighted for each model\n",
        "\n",
        "files = [ #[\"/content/predictions_ANERtestC10_Arbert.txt\",0.0],   #87%\n",
        "           #  [\"/content/predictions_ANERtestC10_Arabert.txt\",0.0],   #82\n",
        "           #[\"/content/predictions_ANERtestC10_MARBERT.txt\",0.0],     #84\n",
        "         #[\"/content/predictions_ANERtestC10_Arabicner.txt\",0.0],   #86\n",
        "         #[\"/content/predictions_ANERtestC10_Qarib.txt\",0.0],      #84\n",
        "         #[\"/content/predictions_ANERtestC10_Asafya.txt\",0.0],    #84\n",
        "         #[\"/content/predictions_ANERtestC10_hatimimoh.txt\",0.0],    #83\n",
        "         [\"/content/predictions_ANERtestC10_Camel-msa-ner.txt\", 0.30], #929\n",
        "        [\"/content/predictions_ANERtestC10_Camel-ca-ner.txt\", 0.16],  #88\n",
        "        [\"/content/predictions_ANERtestC10_Camel-mix-ner.txt\", 0.18],  #924\n",
        "        [\"/content/predictions_ANERtestC10_Multilingual2.txt\",  0.36]  #936\n",
        "         ]\n",
        "\n",
        "\n",
        "\n",
        "file_vote_predictions = Soft_voting(files)\n",
        "\n",
        "files.append([file_vote_predictions,0.0])\n",
        "\n",
        "\n",
        "Allmetrics_soft_voting(files)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-FpYJV514Zpi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JrbM28fEb1j"
      },
      "source": [
        "#New Code for Weighted voting (this code is ony for testing)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted for each model\n",
        "\n",
        "files = [ #[\"/content/predictions_ANERtestC10_Arbert.txt\",0.0],   #87%\n",
        "           #  [\"/content/predictions_ANERtestC10_Arabert.txt\",0.0],   #82\n",
        "           #[\"/content/predictions_ANERtestC10_MARBERT.txt\",0.0],     #84\n",
        "         #[\"/content/predictions_ANERtestC10_Arabicner.txt\",0.0],   #86\n",
        "         #[\"/content/predictions_ANERtestC10_Qarib.txt\",0.0],      #84\n",
        "         #[\"/content/predictions_ANERtestC10_Asafya.txt\",0.0],    #84\n",
        "         #[\"/content/predictions_ANERtestC10_hatimimoh.txt\",0.0],    #83\n",
        "         [\"/content/predictions_ANERtestC10_Camel-msa-ner.txt\", 0.253272], #929\n",
        "        [\"/content/predictions_ANERtestC10_Camel-ca-ner.txt\", 0.23964],  #88\n",
        "        [\"/content/predictions_ANERtestC10_Camel-mix-ner.txt\", 0.251908],  #924\n",
        "        [\"/content/predictions_ANERtestC10_Multilingual2.txt\",  0.25518]  #936\n",
        "         ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "file_vote_predictions = Soft_voting(files)\n",
        "\n",
        "files.append([file_vote_predictions,0.0])\n",
        "\n",
        "\n",
        "Allmetrics_soft_voting(files)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObQSho8tfQe3",
        "outputId": "907f496d-0353-4a6b-d75b-833076d4078b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-msa-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.942\n",
            "F1-score: 0.929\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-ca-ner.txt\n",
            "Precision: 0.872\n",
            "Recall: 0.885\n",
            "F1-score: 0.879\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Camel-mix-ner.txt\n",
            "Precision: 0.916\n",
            "Recall: 0.932\n",
            "F1-score: 0.924\n",
            "\n",
            "*************\n",
            " /content/predictions_ANERtestC10_Multilingual2.txt\n",
            "Precision: 0.923\n",
            "Recall: 0.950\n",
            "F1-score: 0.936\n",
            "\n",
            "*************\n",
            " /content/Compined_Predictions_soft_hard.txt\n",
            "Precision: 0.934\n",
            "Recall: 0.951\n",
            "F1-score: 0.942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHD8W-_zS2lW"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}